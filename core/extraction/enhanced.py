"""
Enhanced Extraction System for ScrapiGen
Handles: Hidden data, multi-level scraping, smart field detection
Goal: 90%+ accuracy on real-world sites
"""

import re
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
import json
from groq import Groq
import os
from dotenv import load_dotenv

# Load .env and ensure GROQ_API_KEY is present before importing modules that instantiate Groq
load_dotenv()

groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))


# ============================================================================
# FEATURE 1: Extract Hidden Data (display:none, collapsed, etc.)
# ============================================================================

def extract_hidden_data(html: str) -> Dict[str, List[str]]:
    """
    Extract data that's in HTML but hidden with CSS/JS
    
    Examples:
    - Phone numbers in display:none dropdowns
    - Emails in data attributes
    - Links in collapsed sections
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    hidden_data = {
        "phones": [],
        "emails": [],
        "hidden_links": [],
        "data_attributes": {}
    }
    
    # 1. Find phone numbers (even if hidden)
    # Look in: href="tel:", data-phone, hidden elements with phone patterns
    
    # Method A: tel: links (most reliable)
    tel_links = soup.find_all('a', href=re.compile(r'tel:', re.I))
    for link in tel_links:
        phone = link.get('href', '').replace('tel:', '').replace('tel://', '').strip()
        if phone and phone not in hidden_data["phones"]:
            hidden_data["phones"].append(phone)
    
    # Method B: Data attributes
    phone_elements = soup.find_all(attrs={'data-p': True, 'data-pdisplay': True})
    for elem in phone_elements:
        # Sometimes phone is encoded in data attributes
        pdisplay = elem.get('data-pdisplay', '')
        if pdisplay:
            # Decode if needed (site-specific, but we can handle common patterns)
            hidden_data["phones"].append(pdisplay)
    
    # Method C: Search all text for phone patterns (last resort)
    phone_pattern = r'[\+\(]?[1-9][0-9 .\-\(\)]{8,}[0-9]'
    all_text = soup.get_text()
    potential_phones = re.findall(phone_pattern, all_text)
    for phone in potential_phones:
        cleaned = phone.strip()
        if len(cleaned) >= 10 and cleaned not in hidden_data["phones"]:
            hidden_data["phones"].append(cleaned)
    
    # 2. Find emails (even if hidden)
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    
    # Method A: mailto: links
    mailto_links = soup.find_all('a', href=re.compile(r'mailto:', re.I))
    for link in mailto_links:
        email = link.get('href', '').replace('mailto:', '').strip()
        if email and email not in hidden_data["emails"]:
            hidden_data["emails"].append(email)
    
    # Method B: Search all text
    potential_emails = re.findall(email_pattern, all_text)
    for email in potential_emails:
        if email not in hidden_data["emails"]:
            hidden_data["emails"].append(email)
    
    # 3. Find hidden links (style="display:none" or class="hidden")
    hidden_elements = soup.find_all(style=re.compile(r'display:\s*none', re.I))
    for elem in hidden_elements:
        links = elem.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if href and href.startswith('http'):
                hidden_data["hidden_links"].append(href)
    
    return hidden_data


# ============================================================================
# FEATURE 2: Multi-Level Scraping (List â†’ Detail Pages)
# ============================================================================

def detect_list_page(html: str) -> Dict:
    """
    Detect if this is a list/directory page with links to detail pages
    
    Returns:
        {
            "is_list": bool,
            "item_links": List[str],
            "link_pattern": str,
            "confidence": float
        }
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # Find all links
    all_links = soup.find_all('a', href=True)
    
    # Group links by pattern
    link_patterns = {}
    for link in all_links:
        href = link.get('href', '')
        
        # Skip navigation/footer links
        skip_patterns = [
            '/login', '/register', '/contact', '/about',
            '/privacy', '/terms', '/search', '#',
            'facebook.com', 'twitter.com', 'linkedin.com'
        ]
        
        if any(skip in href.lower() for skip in skip_patterns):
            continue
        
        # Extract pattern (e.g., /company/123 â†’ /company/)
        pattern = re.sub(r'/[0-9]+', '/{id}', href)
        pattern = re.sub(r'/[a-zA-Z0-9\-_]+$', '/{slug}', pattern)
        
        if pattern not in link_patterns:
            link_patterns[pattern] = []
        
        link_patterns[pattern].append(href)
    
    # Find patterns with multiple links (indicates list page)
    repeated_patterns = {
        pattern: links 
        for pattern, links in link_patterns.items() 
        if len(links) >= 3  # At least 3 similar links
    }
    
    if repeated_patterns:
        # Get the most common pattern
        main_pattern = max(repeated_patterns.items(), key=lambda x: len(x[1]))
        
        return {
            "is_list": True,
            "item_links": main_pattern[1],
            "link_pattern": main_pattern[0],
            "total_items": len(main_pattern[1]),
            "confidence": 0.9
        }
    
    return {
        "is_list": False,
        "item_links": [],
        "link_pattern": None,
        "confidence": 0.0
    }


# ============================================================================
# FEATURE 3: Smart Field Detection (Know What Data Means)
# ============================================================================

def smart_field_detection(html: str, hidden_data: Dict) -> Dict:
    """
    Intelligently detect and categorize data fields
    
    Returns structured data with confidence scores
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    detected_fields = {
        "company_name": None,
        "website": None,
        "phones": [],
        "emails": [],
        "address": None,
        "description": None,
    }
    
    # 1. Company Name (usually in h1, title, or og:title)
    title_candidates = [
        soup.find('h1'),
        soup.find('meta', property='og:title'),
        soup.find('title'),
    ]
    
    for candidate in title_candidates:
        if candidate:
            if candidate.name == 'meta':
                text = candidate.get('content', '').strip()
            else:
                text = candidate.get_text(strip=True)
            
            if text and len(text) > 3:
                detected_fields["company_name"] = text
                break
    
    # 2. Website URL (look for external links, "website" labels)
    website_indicators = [
        soup.find('a', string=re.compile(r'website|site web|visit', re.I)),
        soup.find('a', href=re.compile(r'^https?://(?!.*machineryzone)', re.I)),
        soup.find('a', rel='nofollow', href=True),
    ]
    
    for indicator in website_indicators:
        if indicator:
            href = indicator.get('href', '')
            if href.startswith('http') and 'machineryzone.com' not in href:
                detected_fields["website"] = href
                break
    
    # 3. Phones (use hidden data we found)
    detected_fields["phones"] = hidden_data.get("phones", [])
    
    # 4. Emails (use hidden data we found)
    detected_fields["emails"] = hidden_data.get("emails", [])
    
    # 5. Address (look for common patterns)
    address_patterns = [
        soup.find('address'),
        soup.find(itemprop='address'),
        soup.find(class_=re.compile(r'address|location|addr', re.I)),
    ]
    
    for pattern in address_patterns:
        if pattern:
            detected_fields["address"] = pattern.get_text(strip=True)
            break
    
    # 6. Description (look for meta description or main content)
    desc_candidates = [
        soup.find('meta', attrs={'name': 'description'}),
        soup.find('meta', property='og:description'),
        soup.find('p', class_=re.compile(r'description|intro|about', re.I)),
    ]
    
    for candidate in desc_candidates:
        if candidate:
            if candidate.name == 'meta':
                text = candidate.get('content', '').strip()
            else:
                text = candidate.get_text(strip=True)
            
            if text and len(text) > 20:
                detected_fields["description"] = text[:500]  # Limit length
                break
    
    return detected_fields


# ============================================================================
# FEATURE 4: Hybrid Extraction (Rules + LLM)
# ============================================================================

def hybrid_extract(html: str, prompt: str) -> Dict:
    """
    Hybrid approach: Use rules first, LLM for gaps
    
    This gives 90%+ accuracy by combining:
    1. Fast rule-based extraction (phones, emails, etc.)
    2. LLM for ambiguous/complex data
    """
    
    # Step 1: Extract hidden data (fast, rule-based)
    hidden_data = extract_hidden_data(html)
    
    # Step 2: Smart field detection (fast, rule-based)
    detected_fields = smart_field_detection(html, hidden_data)
    
    # Step 3: Check what's missing
    missing_fields = []
    requested_fields = extract_requested_fields_from_prompt(prompt)
    
    for field in requested_fields:
        field_lower = field.lower()
        
        # Check if we found this field
        if 'phone' in field_lower and not detected_fields["phones"]:
            missing_fields.append(field)
        elif 'email' in field_lower and not detected_fields["emails"]:
            missing_fields.append(field)
        elif 'website' in field_lower and not detected_fields["website"]:
            missing_fields.append(field)
        elif 'company' in field_lower or 'name' in field_lower and not detected_fields["company_name"]:
            missing_fields.append(field)
        elif 'address' in field_lower and not detected_fields["address"]:
            missing_fields.append(field)
    
    # Step 4: Use LLM only for missing fields (saves tokens!)
    if missing_fields:
        print(f"âš¡ Using LLM for missing fields: {missing_fields}")
        
        # Clean HTML for LLM
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()
        
        cleaned_html = str(soup)[:20000]  # Limit size
        
        system_prompt = """Extract ONLY the missing fields. Be precise.
Return JSON with only the fields you can find."""

        user_prompt = f"""HTML:
{cleaned_html}

Extract these missing fields: {', '.join(missing_fields)}

Return JSON."""

        try:
            completion = groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            # llm_result = json.loads(completion.choices[0].message.content)
            
            # # Merge LLM results with detected fields
            # for key, value in llm_result.items():
            #     if value and not detected_fields.get(key):
            #         detected_fields[key] = value
   # Normalize LLM response -> ensure we have a dict before iterating
            raw_content = completion.choices[0].message.content
            if isinstance(raw_content, str):
                try:
                    llm_result = json.loads(raw_content)
                except json.JSONDecodeError:
                    # Try to extract a JSON object substring if the model returned extra text
                    import re
                    m = re.search(r'(\{.*\})', raw_content, re.S)
                    if m:
                        try:
                            llm_result = json.loads(m.group(1))
                        except Exception:
                            llm_result = {}
                    else:
                        llm_result = {}
            else:
                llm_result = raw_content or {}

            # Merge LLM results with detected fields (only if dict)
            if isinstance(llm_result, dict):
                for key, value in llm_result.items():
                    if value and not detected_fields.get(key):
                        detected_fields[key] = value
            elif isinstance(llm_result, list) and llm_result and isinstance(llm_result[0], dict):
                # handle case where model returns a list of objects â€” merge first object
                for key, value in llm_result[0].items():
                    if value and not detected_fields.get(key):
                        detected_fields[key] = value
            else:
                print("âš ï¸ Unexpected LLM response format; skipping merge")
        
        except Exception as e:
            print(f"âš ï¸ LLM extraction failed: {e}")
    
    # Step 5: Return combined result
    return {
        "data": [detected_fields],
        "strategy": "hybrid",
        "rules_found": {k: v for k, v in detected_fields.items() if v},
        "llm_used": len(missing_fields) > 0
    }


def extract_requested_fields_from_prompt(prompt: str) -> List[str]:
    """Parse prompt to understand what user wants"""
    prompt_lower = prompt.lower()
    
    field_map = {
        'company': ['company', 'business', 'name'],
        'phone': ['phone', 'telephone', 'tel', 'number'],
        'email': ['email', 'e-mail', 'mail'],
        'website': ['website', 'site', 'url', 'web'],
        'address': ['address', 'location', 'addr'],
        'description': ['description', 'about', 'info'],
    }
    
    requested = []
    for field, keywords in field_map.items():
        if any(keyword in prompt_lower for keyword in keywords):
            requested.append(field)
    
    return requested


# ============================================================================
# FEATURE 5: Multi-Level Scraper (List â†’ Details)
# ============================================================================

async def scrape_multi_level(
    list_url: str,
    prompt: str,
    max_detail_pages: int = 50,
    fetch_html_func=None
) -> List[Dict]:
    """
    Scrape list page, then scrape each detail page
    
    Example:
        List: https://www.machineryzone.com/
        Details: Each company's page
    
    Returns: Combined data from all detail pages
    """
    
    print(f"ðŸ” Analyzing list page: {list_url}")
    
    # Step 1: Fetch list page
    list_html = await fetch_html_func(list_url)
    
    # Step 2: Detect if it's a list page
    list_detection = detect_list_page(list_html)
    
    if not list_detection["is_list"]:
        print("âš ï¸ Not detected as list page, trying regular extraction")
        return [hybrid_extract(list_html, prompt)]
    
    print(f"âœ… Detected list page with {list_detection['total_items']} items")
    print(f"ðŸ“‹ Pattern: {list_detection['link_pattern']}")
    
    # Step 3: Get all detail page URLs
    detail_urls = list_detection["item_links"][:max_detail_pages]
    
    # Make URLs absolute
    from urllib.parse import urljoin
    detail_urls = [urljoin(list_url, url) for url in detail_urls]
    
    print(f"ðŸš€ Scraping {len(detail_urls)} detail pages...")
    
    # Step 4: Scrape each detail page
    results = []
    
    for i, detail_url in enumerate(detail_urls, 1):
        try:
            print(f"  [{i}/{len(detail_urls)}] {detail_url}")
            
            detail_html = await fetch_html_func(detail_url)
            extracted = hybrid_extract(detail_html, prompt)
            
            # Add source URL
            extracted["data"][0]["source_url"] = detail_url
            
            results.append(extracted["data"][0])
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
            results.append({"source_url": detail_url, "error": str(e)})
    
    print(f"âœ… Scraped {len(results)} detail pages")
    
    return results


# ============================================================================
# CONVENIENCE FUNCTION
# ============================================================================

async def smart_scrape(url: str, prompt: str, fetch_html_func) -> Dict:
    """
    Smart scraping that handles everything automatically
    
    - Detects if list or detail page
    - Extracts hidden data
    - Uses hybrid approach (rules + LLM)
    - Returns clean data
    """
    
    html = await fetch_html_func(url)
    
    # Check if it's a list page
    list_detection = detect_list_page(html)
    
    if list_detection["is_list"]:
        # Multi-level scraping
        results = await scrape_multi_level(url, prompt, fetch_html_func=fetch_html_func)
        return {
            "data": results,
            "strategy": "multi_level",
            "total_items": len(results)
        }
    else:
        # Single page extraction
        return hybrid_extract(html, prompt)
